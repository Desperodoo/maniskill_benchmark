# RLFT: Reinforcement Learning and Flow-based Training

<p align="center">
  <b>A unified framework for robot learning with diffusion/flow policies</b>
</p>

RLFT æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æœºå™¨äººå­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒä»æ¨¡ä»¿å­¦ä¹ åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„å®Œæ•´æµæ°´çº¿ã€‚æ‰€æœ‰ç®—æ³•çš„å…³é”®è¶…å‚æ•°å‡ç»è¿‡ **çº§è”è¶…å‚æ•°æ‰«æ (Cascade Sweep)** ç³»ç»ŸåŒ–è°ƒä¼˜ã€‚

- **Imitation Learning (IL)**: Diffusion Policy, Flow Matching, ShortCut Flow, Consistency Flow, Reflected Flow
- **Offline RL**: CPQL, AWCP, AW-ShortCut Flow, DQC, Offline SAC
- **Online RL**: RLPD (SAC/AWSC), DSRL-SAC, PLD-SAC, ReinFlow

## ğŸ“ é¡¹ç›®ç»“æ„

```
rlft/
â”œâ”€â”€ algorithms/              # ç­–ç•¥å­¦ä¹ ç®—æ³•
â”‚   â”œâ”€â”€ il/                  # æ¨¡ä»¿å­¦ä¹ ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ diffusion_policy.py      # Diffusion Policy (DDPM)
â”‚   â”‚   â”œâ”€â”€ flow_matching.py         # Flow Matching (ODE-based)
â”‚   â”‚   â”œâ”€â”€ shortcut_flow.py         # ShortCut Flow (few-step sampling)
â”‚   â”‚   â”œâ”€â”€ consistency_flow.py      # Consistency Flow
â”‚   â”‚   â””â”€â”€ reflected_flow.py        # Reflected Flow (bounded actions)
â”‚   â”œâ”€â”€ offline_rl/          # ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ cpql.py                  # CPQL (Conservative Policy Q-Learning)
â”‚   â”‚   â”œâ”€â”€ awcp.py                  # AWCP (Advantage-Weighted Conservative Policy)
â”‚   â”‚   â”œâ”€â”€ aw_shortcut_flow.py      # AW-ShortCut Flow
â”‚   â”‚   â”œâ”€â”€ dqc.py                   # DQC (Decoupled Q-Chunking)
â”‚   â”‚   â””â”€â”€ sac.py                   # Offline SAC (å¤šæ­£åˆ™åŒ–: td3bc/awr/iql/cql)
â”‚   â””â”€â”€ online_rl/           # åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•
â”‚       â”œâ”€â”€ sac.py                   # SAC + Action Chunking (RLPD-style)
â”‚       â”œâ”€â”€ awsc.py                  # AWSC (Advantage-Weighted ShortCut Flow)
â”‚       â”œâ”€â”€ reinflow.py              # ReinFlow (PPO + Flow Matching)
â”‚       â”œâ”€â”€ dsrl_sac.py              # DSRL-SAC (SAC in flow noise space)
â”‚       â””â”€â”€ pld_sac.py               # PLD-SAC (SAC in residual action space)
â”‚
â”œâ”€â”€ networks/                # ç¥ç»ç½‘ç»œæ¶æ„
â”‚   â”œâ”€â”€ unet.py              # Conditional 1D U-Net
â”‚   â”œâ”€â”€ velocity.py          # Velocity networks (VelocityUNet1D, ShortCutVelocityUNet1D)
â”‚   â”œâ”€â”€ q_networks.py        # Q-networks (DoubleQ, EnsembleQ, SigmoidQ)
â”‚   â”œâ”€â”€ actors.py            # Actor networks (Gaussian, Temperature)
â”‚   â””â”€â”€ encoders.py          # Visual/State encoders (PlainConv, ResNet)
â”‚
â”œâ”€â”€ buffers/                 # æ•°æ®ç¼“å†²åŒº
â”‚   â”œâ”€â”€ replay_buffer.py     # Off-policy replay buffers (RLPD)
â”‚   â”œâ”€â”€ dsrl_buffer.py       # DSRL æ ‡å‡† MDP buffer
â”‚   â”œâ”€â”€ pld_buffer.py        # PLD online/offline æ··åˆ buffer
â”‚   â”œâ”€â”€ success_buffer.py    # Success-filtered replay buffer (AWSC)
â”‚   â”œâ”€â”€ rollout_buffer.py    # On-policy rollout buffer (PPO/ReinFlow)
â”‚   â””â”€â”€ smdp.py              # SMDP cumulative reward computation
â”‚
â”œâ”€â”€ datasets/                # æ•°æ®é›†åŠ è½½
â”‚   â”œâ”€â”€ maniskill_dataset.py # ManiSkill3 HDF5 demo loading
â”‚   â”œâ”€â”€ carm_dataset.py      # CARM real robot demo loading
â”‚   â””â”€â”€ data_utils.py        # Data utilities & observation encoding
â”‚
â”œâ”€â”€ envs/                    # ç¯å¢ƒå·¥å…·
â”‚   â”œâ”€â”€ make_env.py          # Environment factory
â”‚   â”œâ”€â”€ evaluate.py          # Evaluation utilities
â”‚   â”œâ”€â”€ base_flow_env.py     # Flow ç¯å¢ƒåŸºç±» (å…±äº« obs ç¼–ç é€»è¾‘)
â”‚   â”œâ”€â”€ dsrl_env.py          # DSRL noise-space ç¯å¢ƒåŒ…è£…å™¨
â”‚   â””â”€â”€ pld_env.py           # PLD residual-action ç¯å¢ƒåŒ…è£…å™¨
â”‚
â”œâ”€â”€ offline/                 # ç¦»çº¿è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_maniskill.py   # ManiSkill ä»¿çœŸè®­ç»ƒ (IL + Offline RL)
â”‚   â””â”€â”€ train_carm.py        # CARM çœŸå®æœºå™¨äººè®­ç»ƒ
â”‚
â”œâ”€â”€ online/                  # åœ¨çº¿è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_rlpd.py        # RLPD/AWSC è®­ç»ƒ (Off-policy + demo mixing)
â”‚   â”œâ”€â”€ train_dsrl.py        # DSRL-SAC è®­ç»ƒ (noise-space RL)
â”‚   â”œâ”€â”€ train_pld.py         # PLD-SAC è®­ç»ƒ (residual RL + Cal-QL)
â”‚   â”œâ”€â”€ train_reinflow.py    # ReinFlow è®­ç»ƒ (On-policy PPO)
â”‚   â””â”€â”€ _flow_helpers.py     # Flow è®­ç»ƒå…±äº«å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ scripts/                 # è‡ªåŠ¨åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ run_full_pipeline.sh     # ä¸€é”®ç¯å¢ƒ+æ•°æ®+è®­ç»ƒ
â”‚   â”œâ”€â”€ run_all_algorithms.sh    # å¤š GPU æ‰¹é‡è®­ç»ƒ
â”‚   â”œâ”€â”€ download_demos.sh        # æ¼”ç¤ºæ•°æ®ä¸‹è½½
â”‚   â”œâ”€â”€ replay_demos.sh          # æ•°æ® replay é¢„å¤„ç†
â”‚   â”œâ”€â”€ setup_maniskill_env.sh   # Conda ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ monitor_training.sh      # è®­ç»ƒç›‘æ§
â”‚   â””â”€â”€ sweep/                   # çº§è”è¶…å‚æ•°æ‰«æç³»ç»Ÿ
â”‚       â”œâ”€â”€ common.sh                # é€šç”¨é…ç½®ä¸å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ run_cascade_sweep.sh     # ä¸‰é˜¶æ®µçº§è”æ‰«æä¸»æ§
â”‚       â”œâ”€â”€ sweep_*.sh               # å„ç®—æ³•æ‰«æè„šæœ¬
â”‚       â””â”€â”€ fine/                    # ç²¾ç»†åŒ–æ‰«æè„šæœ¬
â”‚
â””â”€â”€ utils/                   # é€šç”¨å·¥å…·
    â”œâ”€â”€ checkpoint.py        # æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½
    â”œâ”€â”€ ema.py               # EMA (Exponential Moving Average)
    â”œâ”€â”€ flow_wrapper.py      # ShortCut Flow ç­–ç•¥åŠ è½½/æ¨ç†åŒ…è£…
    â”œâ”€â”€ model_factory.py     # æ¨¡å‹æ„å»ºå·¥å‚
    â””â”€â”€ schedulers.py        # å­¦ä¹ ç‡è°ƒåº¦å™¨
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®

```bash
bash rlft/scripts/setup_maniskill_env.sh
conda activate maniskill
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

### æ•°æ®å‡†å¤‡

```bash
bash rlft/scripts/download_demos.sh LiftPegUpright-v1
bash rlft/scripts/replay_demos.sh LiftPegUpright-v1
```

### ç¦»çº¿è®­ç»ƒ (IL / Offline RL)

```bash
# Flow Matching (åŸºç¡€ IL)
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm flow_matching \
    --obs_mode rgb

# ShortCut Flow
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm shortcut_flow \
    --obs_mode rgb

# AW-ShortCut Flow (Offline RL, éœ€è¦å…ˆè®­ç»ƒ ShortCut Flow checkpoint)
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm aw_shortcut_flow \
    --obs_mode rgb
```

### åœ¨çº¿è®­ç»ƒ (Online RL)

ä»¥ä¸‹ä¸‰ç§åœ¨çº¿ RL æ–¹æ³•å‡éœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„ ShortCut Flow checkpointï¼š

```bash
# DSRL-SAC: åœ¨ flow noise space ä¸­è·‘ SAC
python -m rlft.online.train_dsrl \
    --env_id LiftPegUpright-v1 \
    --checkpoint /path/to/shortcut_flow_best.pt

# PLD-SAC: åœ¨ residual action space ä¸­è·‘ SAC + Cal-QL é¢„è®­ç»ƒ
python -m rlft.online.train_pld \
    --env_id LiftPegUpright-v1 \
    --checkpoint /path/to/shortcut_flow_best.pt

# RLPD (SAC + demo mixing)
python -m rlft.online.train_rlpd \
    --env_id LiftPegUpright-v1 \
    --algorithm sac
```

### æ‰¹é‡è®­ç»ƒä¸ç›‘æ§

```bash
# æ‰¹é‡è®­ç»ƒæ‰€æœ‰ç¦»çº¿ç®—æ³•
bash rlft/scripts/run_all_algorithms.sh --full

# çº§è”è¶…å‚æ•°æ‰«æ
bash rlft/scripts/sweep/run_cascade_sweep.sh

# ç›‘æ§è®­ç»ƒ
tensorboard --logdir runs/
```

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### 1. ç¦»çº¿æ¨¡ä»¿å­¦ä¹  (Imitation Learning)

é€šè¿‡ `train_maniskill.py` ç»Ÿä¸€å…¥å£ï¼Œæ”¯æŒ 5 ç§ IL ç®—æ³•ï¼š

```bash
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --demo_path ~/.maniskill/demos/LiftPegUpright-v1/rl/trajectory.rgb.pd_ee_delta_pose.physx_cuda.h5 \
    --algorithm <algorithm> \
    --obs_mode rgb \
    --total_iters 25000
```

| ç®—æ³• | `--algorithm` | æè¿° | Sweep æœ€ä¼˜ success_once |
|------|---------------|------|------------------------|
| Flow Matching | `flow_matching` | ODE-based è¿ç»­æ—¶é—´æµ | 28% |
| ShortCut Flow | `shortcut_flow` | è‡ªé€‚åº”æ­¥é•¿å¿«é€Ÿé‡‡æ · (1-8æ­¥) | 34% |
| Consistency Flow | `consistency_flow` | ä¸€è‡´æ€§æ­£åˆ™åŒ–æµ | **48%** |
| Diffusion Policy | `diffusion_policy` | DDPM-based å¤šæ­¥å»å™ª | 5% |
| Reflected Flow | `reflected_flow` | è¾¹ç•Œåå°„å¤„ç† | â€” |

#### CARM çœŸå®æœºå™¨äºº

```bash
python -m rlft.offline.train_carm \
    --demo_path ~/recorded_data/pick_place \
    --algorithm flow_matching \
    --total_iters 100000
```

---

### 2. ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline RL)

åŒæ ·é€šè¿‡ `train_maniskill.py` å…¥å£ï¼š

```bash
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm <algorithm> \
    --obs_mode rgb \
    --total_iters 25000
```

| ç®—æ³• | `--algorithm` | æè¿° | Sweep æœ€ä¼˜ success_once |
|------|---------------|------|------------------------|
| AW-ShortCut Flow | `aw_shortcut_flow` | Q-weighted ShortCut Flow | **80%** |
| CPQL | `cpql` | Conservative Q-Learning + Flow Policy | 22% |
| AWCP | `awcp` | Advantage-Weighted Consistency Policy | 7% |
| DQC | `dqc` | Decoupled Q-Chunking (dual sigmoid critic) | â€” |
| Offline SAC | `sac` | å¤šæ­£åˆ™åŒ– SAC (td3bc/awr/iql/cql) | â€” |

---

### 3. åœ¨çº¿å¼ºåŒ–å­¦ä¹  (Online RL)

#### DSRL-SAC (noise-space RL)

åœ¨å†»ç»“ ShortCut Flow ç­–ç•¥çš„ **noise space** ä¸­è¿è¡Œ SACã€‚ç¯å¢ƒåŒ…è£…å™¨å†…éƒ¨å°† noise è§£ç ä¸ºçœŸå®åŠ¨ä½œã€‚

```bash
python -m rlft.online.train_dsrl \
    --env_id LiftPegUpright-v1 \
    --checkpoint /path/to/shortcut_flow_best.pt \
    --total_timesteps 1000000
```

æ ¸å¿ƒè®¾è®¡ï¼ˆç» sweep è°ƒä¼˜ï¼‰ï¼š3Ã—2048 MLP + Tanhï¼Œ10 Q-networksï¼ŒUTD=60ï¼Œ`action_magnitude=2.5`ï¼Œ`gamma=0.95`ï¼Œ`target_entropy=-3.5`ã€‚

#### PLD-SAC (residual RL)

åœ¨å†»ç»“ ShortCut Flow ç­–ç•¥çš„ **residual action space** ä¸­è¿è¡Œ SACï¼Œé™„å¸¦ Cal-QL critic é¢„è®­ç»ƒã€‚

```bash
python -m rlft.online.train_pld \
    --env_id LiftPegUpright-v1 \
    --checkpoint /path/to/shortcut_flow_best.pt \
    --total_timesteps 500000
```

æ ¸å¿ƒè®¾è®¡ï¼ˆç» sweep è°ƒä¼˜ï¼‰ï¼š3Ã—1024 MLPï¼Œ5 Q-networksï¼ŒUTD=60ï¼Œ`action_scale=0.3`ï¼Œ`gamma=0.99`ï¼Œ`target_entropy=-3.5`ï¼Œ`init_temperature=0.1`ï¼ŒCal-QL é¢„è®­ç»ƒ 1000 æ­¥ã€‚

#### RLPD (demo mixing RL)

åœ¨çº¿ SAC/AWSC ä¸ç¦»çº¿æ¼”ç¤ºæ•°æ®æ··åˆè®­ç»ƒ (RLPD-style)ã€‚

```bash
# SAC
python -m rlft.online.train_rlpd \
    --env_id LiftPegUpright-v1 \
    --algorithm sac

# AWSC (éœ€è¦é¢„è®­ç»ƒ ShortCut Flow checkpoint)
python -m rlft.online.train_rlpd \
    --env_id LiftPegUpright-v1 \
    --algorithm awsc \
    --pretrain_path runs/shortcut_bc/best.pt
```

æ ¸å¿ƒè®¾è®¡ï¼ˆç» sweep è°ƒä¼˜ï¼‰ï¼š`online_ratio=0.15`ï¼Œ`awsc_beta=50`ï¼Œ`awsc_bc_weight=2.0`ï¼Œ`lr_actor=1e-4`ã€‚

#### ReinFlow (on-policy PPO + Flow)

ä»é¢„è®­ç»ƒ Flow Matching æ¨¡å‹å‡ºå‘ï¼Œç”¨ PPO è¿›è¡Œ on-policy å¾®è°ƒã€‚

```bash
python -m rlft.online.train_reinflow \
    --env_id PushCube-v1 \
    --pretrained_path runs/flow_matching/checkpoint.pt \
    --total_updates 10000
```

æ”¯æŒçš„ Online RL ç®—æ³•æ€»è§ˆï¼š

| ç®—æ³• | è®­ç»ƒè„šæœ¬ | Action Space | åŸºåº§ç­–ç•¥ | æè¿° |
|------|---------|-------------|---------|------|
| DSRL-SAC | `train_dsrl.py` | noise space | ShortCut Flow | SAC in flow noise space |
| PLD-SAC | `train_pld.py` | residual action | ShortCut Flow | Residual SAC + Cal-QL |
| SAC | `train_rlpd.py --algorithm sac` | raw action | â€” | SAC + Action Chunking + Demo Mixing |
| AWSC | `train_rlpd.py --algorithm awsc` | raw action | ShortCut Flow | Q-weighted ShortCut Flow (RLPD) |
| ReinFlow | `train_reinflow.py` | raw action | Flow Matching | PPO + Flow Matching |

---

## ğŸ”§ å…³é”®å‚æ•°è¯´æ˜

> **æ³¨**ï¼šä»¥ä¸‹é»˜è®¤å€¼å‡ä¸ºç»è¿‡è¶…å‚æ•°æ‰«æåçš„æœ€ä¼˜å€¼ï¼Œä¸€èˆ¬æ— éœ€æ‰‹åŠ¨è¦†ç›–ã€‚

### é€šç”¨å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--seed` | 1 (offline) / 42 (online) | éšæœºç§å­ |
| `--cuda` | True | æ˜¯å¦ä½¿ç”¨ GPU |
| `--track` | True | æ˜¯å¦ä½¿ç”¨ WandB è®°å½• |
| `--capture_video` | False | æ˜¯å¦å½•åˆ¶è¯„ä¼°è§†é¢‘ |

### Action Chunking å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--obs_horizon` | 2 | è§‚æµ‹å†å²é•¿åº¦ |
| `--act_horizon` | 8 | æ‰§è¡ŒåŠ¨ä½œé•¿åº¦ |
| `--pred_horizon` | 8 | é¢„æµ‹åŠ¨ä½œé•¿åº¦ (sweep æœ€ä¼˜: 8 > 16) |

### Flow / ShortCut å‚æ•° (Offline)

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--num_flow_steps` | 20 | Flow Matching ODE æ­¥æ•° (sweep: 20 > 10 > 5) |
| `--ema_decay` | 0.9995 | EMA è¡°å‡ç‡ (sweep wave3 æœ€ä¼˜) |
| `--sc_self_consistency_k` | 0.25 | ShortCut consistency batch æ¯”ä¾‹ |
| `--sc_step_size_mode` | fixed | æ­¥é•¿é‡‡æ ·æ¨¡å¼ (fixed > power2 > uniform) |
| `--sc_fixed_step_size` | 0.15 | å›ºå®šæ­¥é•¿å€¼ (sweep wave3 æœ€ä¼˜) |
| `--sc_num_inference_steps` | 8 | æ¨ç†é‡‡æ ·æ­¥æ•° |

### Offline RL å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--alpha` | 0.0005 | CPQL ç†µç³»æ•° (sweep wave3 æœ€ä¼˜) |
| `--beta` | 10.0 | AWR/AWAC æ¸©åº¦ |
| `--reward_scale` | 0.05 | å¥–åŠ±ç¼©æ”¾å› å­ (sweep wave3 æœ€ä¼˜) |
| `--weight_clip` | 200.0 | AWR æƒé‡è£å‰ª (sweep wave3 æœ€ä¼˜) |
| `--consistency_weight` | 0.3 | ä¸€è‡´æ€§æ­£åˆ™æƒé‡ |

### DSRL-SAC å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--action_magnitude` | 2.5 | Noise space è¾¹ç•Œ $[-\text{mag}, +\text{mag}]$ |
| `--utd_ratio` | 60 | Update-to-Data ratio (sweep æœ€å…³é”®å‚æ•°) |
| `--num_qs` | 10 | Ensemble Q-network æ•°é‡ |
| `--gamma` | 0.95 | æŠ˜æ‰£å› å­ (åŒ¹é… ~12 RL steps/episode) |
| `--target_entropy` | -3.5 | ç›®æ ‡ç†µ (sweep: -3.5 >> auto -56) |
| `--log_std_init` | -5.0 | Actor åˆå§‹ log-std (ä¿å®ˆæ¢ç´¢) |
| `--layer_size` | 2048 | MLP å±‚å®½ |

### PLD-SAC å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--action_scale` | 0.3 | Residual action è¾¹ç•Œ $[-\xi, +\xi]$ |
| `--utd_ratio` | 60 | Update-to-Data ratio |
| `--num_qs` | 5 | Ensemble Q-network æ•°é‡ |
| `--gamma` | 0.99 | æŠ˜æ‰£å› å­ |
| `--target_entropy` | -3.5 | ç›®æ ‡ç†µ |
| `--init_temperature` | 0.1 | åˆå§‹æ¸©åº¦ (è¿‘ç¡®å®šæ€§å¯åŠ¨) |
| `--learning_rate` | 1e-4 | å­¦ä¹ ç‡ (é˜²æ­¢é«˜ UTD ä¸‹ Q å‘æ•£) |
| `--layer_size` | 1024 | MLP å±‚å®½ |
| `--calql_pretrain_steps` | 1000 | Cal-QL critic é¢„è®­ç»ƒæ­¥æ•° |
| `--calql_alpha` | 0.0 | Cal-QL ä¿å®ˆç³»æ•° (sweep: 0 æœ€ä¼˜) |
| `--online_ratio` | 1.0 | åœ¨çº¿æ•°æ®æ¯”ä¾‹ (1.0=çº¯åœ¨çº¿) |

### RLPD / AWSC å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--online_ratio` | 0.15 | åœ¨çº¿æ•°æ®æ¯”ä¾‹ (sweep v3/v4 æœ€ä¼˜) |
| `--lr_actor` | 1e-4 | Actor å­¦ä¹ ç‡ (sweep: 1e-4 é˜²æ­¢ç¾éš¾é—å¿˜) |
| `--awsc_beta` | 50.0 | Advantage weighting æ¸©åº¦ (sweep v2-v4 æœ€ä¼˜) |
| `--awsc_bc_weight` | 2.0 | Flow BC æŸå¤±æƒé‡ (sweep v2: é”šå®šé¢„è®­ç»ƒç­–ç•¥) |
| `--awsc_advantage_mode` | per_state_v | Advantage è®¡ç®—æ¨¡å¼ (sweep: ä¼˜äº batch_mean) |
| `--num_qs` | 10 | Ensemble Q-network æ•°é‡ |
| `--num_min_qs` | 2 | Min-Q é‡‡æ ·æ•° |

### DQC å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--backup_horizon` | 16 | Chunk critic horizon $h$ |
| `--kappa_b` | 0.9 | V-network backup expectile |
| `--kappa_d` | 0.8 | Action critic distillation expectile |
| `--best_of_n` | 32 | Best-of-N æ¨ç†å€™é€‰æ•° |

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### è®­ç»ƒæµæ°´çº¿ (ä¸‰é˜¶æ®µ)

```
Stage 1: IL (çº¯ BC)           Stage 2: Offline RL (Q-weighted BC)     Stage 3: Online RL (å¾®è°ƒ)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Flow Matching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     AW-ShortCut Flow                        DSRL-SAC  (noise space)
ShortCut Flow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     CPQL                                    PLD-SAC   (residual space)
Consistency Flow â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶  AWCP                                â”€â”€â–¶ RLPD/AWSC (demo mixing)
Diffusion Policy â”€â”€â”€â”€â”€â”€â”€â”¤     DQC                                     ReinFlow  (PPO)
Reflected Flow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Offline SAC
```

### ç®—æ³•ç»§æ‰¿å…³ç³»

```
nn.Module
â”œâ”€â”€ DiffusionPolicyAgent
â”œâ”€â”€ FlowMatchingAgent
â”‚   â””â”€â”€ ShortCutFlowAgent
â”‚       â”œâ”€â”€ ConsistencyFlowAgent
â”‚       â””â”€â”€ ReflectedFlowAgent
â”œâ”€â”€ CPQLAgent
â”‚   â””â”€â”€ AWCPAgent
â”‚       â””â”€â”€ AWShortCutFlowAgent
â”œâ”€â”€ DQCAgent                    # Dual sigmoid critic + flow actor
â”œâ”€â”€ OfflineSACAgent             # Multi-regularization (td3bc/awr/iql/cql)
â”œâ”€â”€ SACAgent                    # RLPD-style online SAC
â”œâ”€â”€ AWSCAgent                   # Online Q-weighted ShortCut Flow
â”œâ”€â”€ DSRLSACAgent                # Noise-space SAC (MLP actor/critic)
â”œâ”€â”€ PLDSACAgent                 # Residual-space SAC (MLP + Cal-QL)
â””â”€â”€ ReinFlowAgent               # PPO + Flow Matching
```

### ç½‘ç»œæ¶æ„

```
Visual Encoder (PlainConv/ResNet)
        â”‚
        â–¼
    obs_features (B, T * feature_dim)
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                                              â–¼
VelocityUNet1D / ShortCutVelocityUNet1D   Q-Networks (Double/Ensemble/Sigmoid)
        â”‚                                              â”‚
        â–¼                                              â–¼
   actions (B, pred_horizon, act_dim)            Q-values (B, 1)


DSRL-SAC / PLD-SAC æ¶æ„ (MLP):
        obs_features
        â”‚          â”‚
        â–¼          â–¼
   DSRLActor   DSRLCritic / PLDCritic
   (MLP+Tanh)  (Ensemble MLP, 10/5 Q-nets)
        â”‚          â”‚
        â–¼          â–¼
   noise/residual  Q(obs, noise/residual)
        â”‚
        â–¼
   Flow Policy (frozen) â†’ real actions
```

### SMDP (Semi-Markov Decision Process) å…¬å¼

å¯¹äº action chunk é•¿åº¦ $\tau$ï¼š
- **ç´¯ç§¯å¥–åŠ±**: $R_t^{(\tau)} = \sum_{i=0}^{\tau-1} \gamma^i r_{t+i}$
- **æŠ˜æ‰£å› å­**: $\gamma^\tau$
- **Bellman æ–¹ç¨‹**: $Q(s_t, a_{t:t+\tau}) = R_t^{(\tau)} + \gamma^\tau (1 - d) Q(s_{t+\tau}, a')$

### DQC å…¬å¼ (Decoupled Q-Chunking)

å››ç½‘ç»œåˆ†é˜¶æ®µè®­ç»ƒ:
1. **Value**: $L_V = \mathbb{E}[|\kappa_b - \mathbf{1}(Q^{h_a}_\text{target} - V < 0)| \cdot (Q^{h_a}_\text{target} - V)^2]$
2. **Chunk Critic**: $L_{Q^h} = \text{BCE}(Q^h_\text{logit}, \sigma(\text{target}))$, target from $V$-bootstrap
3. **Action Critic**: $L_{Q^{h_a}} = \mathbb{E}[|\kappa_d - \mathbf{1}(Q^h - Q^{h_a} < 0)| \cdot (Q^h - Q^{h_a})^2]$
4. **Actor**: $L_\pi = \mathbb{E}[\|v_\theta(x_t, t) - (x_1 - x_0)\|^2]$ (çº¯ Flow Matching BC, best-of-N æ¨ç†)

---

## ğŸ” è¶…å‚æ•°æ‰«æç³»ç»Ÿ

æœ¬é¡¹ç›®å†…ç½®äº†**ä¸‰é˜¶æ®µçº§è”è¶…å‚æ•°æ‰«æç³»ç»Ÿ** (`scripts/sweep/`)ï¼ŒæŒ‰ä¾èµ–é¡ºåºè‡ªåŠ¨è°ƒä¼˜ï¼š

```
é˜¶æ®µ 1 (åŸºç¡€ IL):      flow_matching, diffusion_policy
         â†“ ç»§æ‰¿æœ€ä¼˜å‚æ•°
é˜¶æ®µ 2 (ä¾èµ– IL):      consistency_flow, shortcut_flow, reflected_flow
         â†“ ç»§æ‰¿æœ€ä¼˜å‚æ•°
é˜¶æ®µ 3 (Offline RL):   cpql, awcp, aw_shortcut_flow
```

```bash
# è¿è¡Œå®Œæ•´çº§è”æ‰«æ
bash rlft/scripts/sweep/run_cascade_sweep.sh

# åªè¿è¡ŒæŸä¸€é˜¶æ®µ
bash rlft/scripts/sweep/run_cascade_sweep.sh --stage 1

# åªè¿è¡ŒæŸä¸ªç®—æ³•
bash rlft/scripts/sweep/run_cascade_sweep.sh --algorithm awcp

# ç²¾ç»†åŒ–æ‰«æ (åœ¨ç¬¬ä¸€è½®æœ€ä¼˜ç‚¹é™„è¿‘åŠ å¯†æœç´¢)
bash rlft/scripts/sweep/run_cascade_sweep.sh --fine-sweep

# æŸ¥çœ‹è¿›åº¦
bash rlft/scripts/sweep/run_cascade_sweep.sh --status

# é‡è·‘å¤±è´¥çš„å®éªŒ
bash rlft/scripts/sweep/run_cascade_sweep.sh --retry-failed
```

æ”¯æŒå¤š GPU å¹¶è¡Œ/ä¸²è¡Œæ¨¡å¼ã€CUDA é”™è¯¯è‡ªåŠ¨é‡è¯•ã€WandB æ—¥å¿—é›†æˆã€‚

---

## ğŸ“Š å®éªŒç»“æœè®°å½•

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ `runs/` ç›®å½•ï¼š

```
runs/
â”œâ”€â”€ {exp_name}__{timestamp}/
â”‚   â”œâ”€â”€ config.json          # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ events.out.tfevents  # TensorBoard æ—¥å¿—
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ best.pt          # æœ€ä½³æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ step_*.pt        # å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹
â”‚   â”‚   â””â”€â”€ final.pt         # æœ€ç»ˆæ¨¡å‹
â”‚   â””â”€â”€ videos/              # è¯„ä¼°è§†é¢‘
```

```bash
tensorboard --logdir runs/
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- **Diffusion Policy**: [Chi et al., RSS 2023](https://diffusion-policy.cs.columbia.edu/)
- **Flow Matching**: [Lipman et al., ICLR 2023](https://arxiv.org/abs/2210.02747)
- **ShortCut Flow**: [Frans et al., 2024](https://arxiv.org/abs/2410.12557)
- **RLPD**: [Ball et al., ICML 2023](https://arxiv.org/abs/2302.02948)
- **ReinFlow**: [Ding et al., 2024](https://arxiv.org/abs/2402.14262)
- **CPQL**: [Nakamoto et al., ICLR 2024](https://arxiv.org/abs/2310.07297)
- **DSRL**: [Wagen et al., 2024](https://github.com/ajwagen/dsrl)
- **PLD**: [Xiao et al., 2024](https://arxiv.org/abs/2511.00091)
- **DQC**: [Li, Park, Levine, 2025](https://arxiv.org/abs/2512.10926)
- **Cal-QL**: [Nakamoto et al., NeurIPS 2024](https://arxiv.org/abs/2303.05479)
- **IQL**: [Kostrikov et al., ICLR 2022](https://arxiv.org/abs/2110.06169)

---

## ğŸ“ License

MIT License
