# RLFT: Reinforcement Learning and Flow-based Training

<p align="center">
  <b>A unified framework for robot learning with diffusion/flow policies</b>
</p>

RLFT æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æœºå™¨äººå­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒï¼š
- **Imitation Learning (IL)**: Diffusion Policy, Flow Matching, ShortCut Flow, Consistency Flow
- **Offline Reinforcement Learning**: CPQL, AWCP, AW-ShortCut Flow
- **Online Reinforcement Learning**: SAC, RLPD, ReinFlow, AWSC

## ğŸ“ é¡¹ç›®ç»“æ„

```
rlft/
â”œâ”€â”€ algorithms/          # ç­–ç•¥å­¦ä¹ ç®—æ³•
â”‚   â”œâ”€â”€ il/              # æ¨¡ä»¿å­¦ä¹ ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ diffusion_policy.py    # Diffusion Policy (DDPM)
â”‚   â”‚   â”œâ”€â”€ flow_matching.py       # Flow Matching (ODE-based)
â”‚   â”‚   â”œâ”€â”€ shortcut_flow.py       # ShortCut Flow (few-step sampling)
â”‚   â”‚   â”œâ”€â”€ consistency_flow.py    # Consistency Flow
â”‚   â”‚   â””â”€â”€ reflected_flow.py      # Reflected Flow (bounded actions)
â”‚   â”œâ”€â”€ offline_rl/      # ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ cpql.py                # CPQL (Conservative Policy Q-Learning)
â”‚   â”‚   â”œâ”€â”€ awcp.py                # AWCP (Advantage-Weighted Conservative Policy)
â”‚   â”‚   â””â”€â”€ aw_shortcut_flow.py    # AW-ShortCut Flow
â”‚   â””â”€â”€ online_rl/       # åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•
â”‚       â”œâ”€â”€ sac.py                 # SAC (Soft Actor-Critic)
â”‚       â”œâ”€â”€ reinflow.py            # ReinFlow (PPO + Flow Matching)
â”‚       â””â”€â”€ awsc.py                # AWSC (Advantage-Weighted ShortCut Flow)
â”‚
â”œâ”€â”€ networks/            # ç¥ç»ç½‘ç»œæ¶æ„
â”‚   â”œâ”€â”€ unet.py          # Conditional 1D U-Net
â”‚   â”œâ”€â”€ velocity.py      # Velocity networks (VelocityUNet1D, ShortCutVelocityUNet1D)
â”‚   â”œâ”€â”€ q_networks.py    # Q-networks (DoubleQ, EnsembleQ)
â”‚   â”œâ”€â”€ actors.py        # Actor networks (Gaussian, Temperature)
â”‚   â””â”€â”€ encoders.py      # Visual/State encoders (PlainConv, ResNet)
â”‚
â”œâ”€â”€ buffers/             # æ•°æ®ç¼“å†²åŒº
â”‚   â”œâ”€â”€ replay_buffer.py    # Off-policy replay buffers
â”‚   â”œâ”€â”€ success_buffer.py   # Success-filtered replay buffer
â”‚   â”œâ”€â”€ rollout_buffer.py   # On-policy rollout buffer (PPO)
â”‚   â””â”€â”€ smdp.py             # SMDP cumulative reward computation
â”‚
â”œâ”€â”€ datasets/            # æ•°æ®é›†åŠ è½½
â”‚   â”œâ”€â”€ maniskill_dataset.py   # ManiSkill3 HDF5 demo loading
â”‚   â”œâ”€â”€ carm_dataset.py        # CARM real robot demo loading
â”‚   â””â”€â”€ data_utils.py          # Data utilities
â”‚
â”œâ”€â”€ envs/                # ç¯å¢ƒå·¥å…·
â”‚   â”œâ”€â”€ make_env.py      # Environment factory
â”‚   â””â”€â”€ evaluate.py      # Evaluation utilities
â”‚
â”œâ”€â”€ offline/             # ç¦»çº¿è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_carm.py       # CARM çœŸå®æœºå™¨äººè®­ç»ƒ
â”‚   â””â”€â”€ train_maniskill.py  # ManiSkill ä»¿çœŸè®­ç»ƒ
â”‚
â”œâ”€â”€ online/              # åœ¨çº¿è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_rlpd.py       # RLPD/AWSC è®­ç»ƒ (Off-policy)
â”‚   â””â”€â”€ train_reinflow.py   # ReinFlow è®­ç»ƒ (On-policy)
â”‚
â”œâ”€â”€ roboreward/          # RoboReward æ ‡æ³¨å·¥å…·
â”‚
â”œâ”€â”€ tests/               # æµ‹è¯•ç”¨ä¾‹
â”‚
â””â”€â”€ utils/               # é€šç”¨å·¥å…·
    â”œâ”€â”€ checkpoint.py    # æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½
    â”œâ”€â”€ ema.py           # EMA (Exponential Moving Average)
    â””â”€â”€ schedulers.py    # å­¦ä¹ ç‡è°ƒåº¦å™¨
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

æˆ‘ä»¬æä¾›äº†å®Œæ•´çš„è‡ªåŠ¨åŒ–è„šæœ¬ï¼Œä¸€é”®å®Œæˆç¯å¢ƒé…ç½®ã€æ•°æ®ä¸‹è½½ã€æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒï¼š

#### Step 1: é…ç½®ç¯å¢ƒ

```bash
# åˆ›å»ºåä¸º 'maniskill' çš„ conda ç¯å¢ƒ
bash scripts/setup_maniskill_env.sh

# æ¿€æ´»ç¯å¢ƒ
conda activate maniskill
```

#### Step 2: ä¸‹è½½æ¼”ç¤ºæ•°æ®

```bash
# ä¸‹è½½ LiftPegUpright-v1 ä»»åŠ¡çš„æ¼”ç¤º
bash scripts/download_demos.sh LiftPegUpright-v1

# ä¸‹è½½å¤šä¸ªä»»åŠ¡
bash scripts/download_demos.sh LiftPegUpright-v1 PickCube-v1 PushCube-v1
```

#### Step 3: Replay ç”Ÿæˆè®­ç»ƒæ•°æ®

```bash
# Replay ç”Ÿæˆ RGB å’Œ State ä¸¤ç§è§‚æµ‹æ¨¡å¼çš„æ•°æ®é›†
# ä½¿ç”¨ physx_cuda åç«¯ï¼Œä¿å­˜ sparse å¥–åŠ±
bash scripts/replay_demos.sh LiftPegUpright-v1

# æŒ‡å®šæ§åˆ¶æ¨¡å¼å’Œå¹¶è¡Œç¯å¢ƒæ•°
bash scripts/replay_demos.sh LiftPegUpright-v1 pd_ee_delta_pose 64
```

#### Step 4: æ‰¹é‡è®­ç»ƒ

```bash
# å¿«é€ŸéªŒè¯æ‰€æœ‰ç®—æ³• (5000 æ­¥)
bash scripts/run_all_algorithms.sh --quick

# å®Œæ•´è®­ç»ƒæ‰€æœ‰ç®—æ³• (100ä¸‡æ­¥)
bash scripts/run_all_algorithms.sh --full

# æŒ‡å®š GPU å’Œç®—æ³•
bash scripts/run_all_algorithms.sh --quick --gpus 0,1,2,3 --algorithms flow_matching,cpql

# åªè®­ç»ƒ RGB è§‚æµ‹
bash scripts/run_all_algorithms.sh --quick --obs-mode rgb

# é¢„è§ˆå‘½ä»¤ï¼ˆä¸æ‰§è¡Œï¼‰
bash scripts/run_all_algorithms.sh --quick --dry-run
```

#### Step 5: ç›‘æ§è®­ç»ƒ

```bash
# å¯åŠ¨ç›‘æ§ç•Œé¢
bash scripts/monitor_training.sh logs/training_<timestamp>

# æŸ¥çœ‹ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹å•ä¸ªä»»åŠ¡æ—¥å¿—
tail -f logs/training_<timestamp>/LiftPegUpright-v1_flow_matching_rgb.log

# ç»ˆæ­¢æ‰€æœ‰è®­ç»ƒ
pkill -f 'rlft.offline.train_maniskill'
```

### æ‰‹åŠ¨å®‰è£…ä¾èµ–

```bash
# åˆ›å»º conda ç¯å¢ƒ
conda create -n maniskill python=3.10
conda activate maniskill

# å®‰è£… PyTorch (CUDA 12.1)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# å®‰è£… ManiSkill3
pip install mani-skill

# å®‰è£…å…¶ä»–ä¾èµ–
pip install tyro diffusers wandb tensorboard h5py einops scikit-learn opencv-python
```

### ç¯å¢ƒå˜é‡

```bash
# è®¾ç½® PYTHONPATH
export PYTHONPATH=$PYTHONPATH:/path/to/rl-vla
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### 1. ç¦»çº¿æ¨¡ä»¿å­¦ä¹  (Imitation Learning)

#### ManiSkill ä»¿çœŸç¯å¢ƒ

```bash
# Flow Matching
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --demo_path ~/.maniskill/demos/LiftPegUpright-v1/rl/trajectory.state.pd_ee_delta_pose.physx_cuda.h5 \
    --algorithm flow_matching \
    --obs_mode state \
    --total_iters 100000

# ShortCut Flow (few-step sampling)
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --demo_path ~/.maniskill/demos/LiftPegUpright-v1/rl/trajectory.rgb.pd_ee_delta_pose.physx_cuda.h5 \
    --algorithm shortcut_flow \
    --obs_mode rgb \
    --total_iters 100000

# Diffusion Policy
python -m rlft.offline.train_maniskill \
    --env_id PickCube-v1 \
    --algorithm diffusion_policy \
    --num_diffusion_iters 100
```

#### CARM çœŸå®æœºå™¨äºº

```bash
# Flow Matching
python -m rlft.offline.train_carm \
    --demo_path ~/recorded_data/pick_place \
    --algorithm flow_matching \
    --total_iters 100000

# ShortCut Flow
python -m rlft.offline.train_carm \
    --demo_path ~/recorded_data/pick_place \
    --algorithm shortcut_flow \
    --max_denoising_steps 8
```

æ”¯æŒçš„ IL ç®—æ³•ï¼š
| ç®—æ³• | `--algorithm` | æè¿° |
|------|---------------|------|
| Diffusion Policy | `diffusion_policy` | DDPM-based, éœ€è¦å¤šæ­¥å»å™ª |
| Flow Matching | `flow_matching` | ODE-based, è¿ç»­æ—¶é—´æµ |
| ShortCut Flow | `shortcut_flow` | å¿«é€Ÿé‡‡æ · (1-8æ­¥) |
| Consistency Flow | `consistency_flow` | ä¸€è‡´æ€§æ¨¡å‹ |
| Reflected Flow | `reflected_flow` | è¾¹ç•Œåå°„å¤„ç† |

---

### 2. ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline RL)

```bash
# CPQL (Conservative Policy Q-Learning)
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm cpql \
    --obs_mode state \
    --lr_critic 3e-4

# AWCP (Advantage-Weighted Conservative Policy)
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm awcp \
    --awac_beta 10.0

# AW-ShortCut Flow
python -m rlft.offline.train_maniskill \
    --env_id LiftPegUpright-v1 \
    --algorithm aw_shortcut_flow \
    --awac_beta 10.0 \
    --shortcut_weight 0.3
```

æ”¯æŒçš„ Offline RL ç®—æ³•ï¼š
| ç®—æ³• | `--algorithm` | æè¿° |
|------|---------------|------|
| CPQL | `cpql` | Conservative Q-Learning with Flow Policy |
| AWCP | `awcp` | Advantage-Weighted with Conservative Policy |
| AW-ShortCut | `aw_shortcut_flow` | Q-weighted ShortCut Flow |

---

### 3. åœ¨çº¿å¼ºåŒ–å­¦ä¹  (Online RL)

#### RLPD (Off-policy, æ··åˆåœ¨çº¿/ç¦»çº¿æ•°æ®)

```bash
# SAC (é»˜è®¤)
python -m rlft.online.train_rlpd \
    --env_id PickCube-v1 \
    --demo_path ~/.maniskill/demos/PickCube-v1/trajectory.state.h5 \
    --algorithm sac \
    --obs_mode state \
    --total_timesteps 1000000 \
    --online_ratio 0.5

# AWSC (Advantage-Weighted ShortCut Flow)
python -m rlft.online.train_rlpd \
    --env_id PickCube-v1 \
    --algorithm awsc \
    --pretrain_path runs/shortcut_bc/best.pt \
    --awsc_beta 10.0 \
    --awsc_bc_weight 1.0 \
    --awsc_shortcut_weight 0.3 \
    --total_timesteps 1000000
```

#### ReinFlow (On-policy, PPO + Flow)

```bash
# ä»é¢„è®­ç»ƒ Flow Matching æ¨¡å‹å¾®è°ƒ
python -m rlft.online.train_reinflow \
    --env_id PushCube-v1 \
    --pretrained_path runs/flow_matching/checkpoint.pt \
    --obs_mode state \
    --total_updates 10000 \
    --lr 1e-6
```

æ”¯æŒçš„ Online RL ç®—æ³•ï¼š
| ç®—æ³• | è„šæœ¬ | æè¿° |
|------|------|------|
| SAC | `train_rlpd.py --algorithm sac` | Soft Actor-Critic + Action Chunking |
| AWSC | `train_rlpd.py --algorithm awsc` | Q-weighted ShortCut Flow (RLPD style) |
| ReinFlow | `train_reinflow.py` | PPO + Flow Matching |

---

## ğŸ”§ å…³é”®å‚æ•°è¯´æ˜

### é€šç”¨å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--seed` | 1 | éšæœºç§å­ |
| `--cuda` | True | æ˜¯å¦ä½¿ç”¨ GPU |
| `--track` | False | æ˜¯å¦ä½¿ç”¨ WandB è®°å½• |
| `--capture_video` | True | æ˜¯å¦å½•åˆ¶è¯„ä¼°è§†é¢‘ |

### Action Chunking å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--obs_horizon` | 2 | è§‚æµ‹å†å²é•¿åº¦ |
| `--act_horizon` | 8 | æ‰§è¡ŒåŠ¨ä½œé•¿åº¦ |
| `--pred_horizon` | 16 | é¢„æµ‹åŠ¨ä½œé•¿åº¦ |

### Flow/ShortCut å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--max_denoising_steps` | 8 | ShortCut Flow æœ€å¤§æ­¥æ•° |
| `--num_inference_steps` | 8 | æ¨ç†é‡‡æ ·æ­¥æ•° |
| `--shortcut_weight` | 0.3 | ShortCut ä¸€è‡´æ€§æŸå¤±æƒé‡ |

### AWSC ç‰¹æœ‰å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `--awsc_beta` | 10.0 | Advantage weighting æ¸©åº¦ |
| `--awsc_bc_weight` | 1.0 | Flow BC æŸå¤±æƒé‡ |
| `--awsc_filter_policy_data` | False | æ˜¯å¦è¿‡æ»¤ä½ advantage æ ·æœ¬ |
| `--awsc_advantage_threshold` | 0.0 | Advantage è¿‡æ»¤é˜ˆå€¼ |
| `--pretrain_path` | None | é¢„è®­ç»ƒæ£€æŸ¥ç‚¹è·¯å¾„ |

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç®—æ³•ç»§æ‰¿å…³ç³»

```
nn.Module
â”œâ”€â”€ DiffusionPolicyAgent
â”œâ”€â”€ FlowMatchingAgent
â”‚   â””â”€â”€ ShortCutFlowAgent
â”‚       â”œâ”€â”€ ConsistencyFlowAgent
â”‚       â””â”€â”€ ReflectedFlowAgent
â”œâ”€â”€ CPQLAgent
â”‚   â””â”€â”€ AWCPAgent
â”‚       â””â”€â”€ AWShortCutFlowAgent
â”œâ”€â”€ SACAgent
â”œâ”€â”€ ReinFlowAgent
â””â”€â”€ AWSCAgent
```

### ç½‘ç»œæ¶æ„

```
Visual Encoder (PlainConv/ResNet)
        â”‚
        â–¼
    obs_features (B, T * feature_dim)
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                                  â–¼
VelocityUNet1D / ShortCutVelocityUNet1D   Q-Networks (Double/Ensemble)
        â”‚                                  â”‚
        â–¼                                  â–¼
   actions (B, pred_horizon, act_dim)    Q-values (B, 1)
```

### SMDP (Semi-Markov Decision Process) å…¬å¼

å¯¹äº action chunk é•¿åº¦ Ï„ï¼š
- **ç´¯ç§¯å¥–åŠ±**: $R_t^{(\tau)} = \sum_{i=0}^{\tau-1} \gamma^i r_{t+i}$
- **æŠ˜æ‰£å› å­**: $\gamma^\tau$
- **Bellman æ–¹ç¨‹**: $Q(s_t, a_{t:t+\tau}) = R_t^{(\tau)} + \gamma^\tau (1 - d) Q(s_{t+\tau}, a')$

---

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest rlft/tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest rlft/tests/test_awsc.py -v
pytest rlft/tests/test_awsc_rlpd.py -v
```

---

## ğŸ“Š å®éªŒç»“æœè®°å½•

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ `runs/` ç›®å½•ï¼š

```
runs/
â”œâ”€â”€ {exp_name}__{timestamp}/
â”‚   â”œâ”€â”€ config.json          # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ events.out.tfevents  # TensorBoard æ—¥å¿—
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ best.pt          # æœ€ä½³æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ step_*.pt        # å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹
â”‚   â”‚   â””â”€â”€ final.pt         # æœ€ç»ˆæ¨¡å‹
â”‚   â””â”€â”€ videos/              # è¯„ä¼°è§†é¢‘ (å¦‚æœ capture_video=True)
```

æŸ¥çœ‹ TensorBoardï¼š
```bash
tensorboard --logdir runs/
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- **Diffusion Policy**: [Chi et al., RSS 2023](https://diffusion-policy.cs.columbia.edu/)
- **Flow Matching**: [Lipman et al., ICLR 2023](https://arxiv.org/abs/2210.02747)
- **ShortCut Flow**: [Frans et al., 2024](https://arxiv.org/abs/2410.12557)
- **RLPD**: [Ball et al., ICML 2023](https://arxiv.org/abs/2302.02948)
- **ReinFlow**: [Ding et al., 2024](https://arxiv.org/abs/2402.14262)
- **CPQL**: [Nakamoto et al., ICLR 2024](https://arxiv.org/abs/2310.07297)

---

## ğŸ“ License

MIT License
