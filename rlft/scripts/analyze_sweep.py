#!/usr/bin/env python3
"""
è¶…å‚æ•°æ‰«æç»“æœåˆ†æè„šæœ¬

åŠŸèƒ½ï¼š
1. æ”¶é›†æ‰€æœ‰å®éªŒçš„è®­ç»ƒæŒ‡æ ‡ (WandB / TensorBoard / æ—¥å¿—)
2. ä¸ºæ¯ä¸ªç®—æ³•æ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•°é…ç½®
3. ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–

ç”¨æ³•ï¼š
    python scripts/analyze_sweep.py --log_dir logs/sweep_xxx
    python scripts/analyze_sweep.py --wandb_project ManiSkill-Sweep
"""

import argparse
import json
import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import glob

import pandas as pd
import numpy as np


def parse_args():
    parser = argparse.ArgumentParser(description="åˆ†æè¶…å‚æ•°æ‰«æç»“æœ")
    parser.add_argument("--log_dir", type=str, help="æ‰«ææ—¥å¿—ç›®å½•")
    parser.add_argument("--wandb_project", type=str, help="WandB é¡¹ç›®åç§°")
    parser.add_argument("--wandb_entity", type=str, default=None, help="WandB å®ä½“")
    parser.add_argument("--output_dir", type=str, default=None, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--metric", type=str, default="eval/success_once", 
                        help="ç”¨äºæ¯”è¾ƒçš„ä¸»æŒ‡æ ‡")
    parser.add_argument("--secondary_metrics", type=str, nargs="*",
                        default=["eval/success_at_end", "eval/return"],
                        help="æ¬¡è¦æŒ‡æ ‡")
    parser.add_argument("--top_k", type=int, default=3, help="æ¯ä¸ªç®—æ³•æ˜¾ç¤º Top K")
    parser.add_argument("--format", type=str, choices=["table", "json", "csv", "all"],
                        default="all", help="è¾“å‡ºæ ¼å¼")
    # æ–°å¢é€‰é¡¹
    parser.add_argument("--algorithm", type=str, default=None,
                        help="åªåˆ†ææŒ‡å®šç®—æ³•")
    parser.add_argument("--export_best", action="store_true",
                        help="å¯¼å‡ºæœ€ä¼˜å‚æ•°ä¸º shell è„šæœ¬")
    parser.add_argument("--recursive", action="store_true",
                        help="é€’å½’æœç´¢å­ç›®å½•ä¸­çš„æ—¥å¿—")
    return parser.parse_args()


# =============================================================================
# æ•°æ®æ”¶é›†
# =============================================================================

def load_running_tasks(log_dir: str) -> Dict[str, Dict[str, Any]]:
    """ä» running_tasks.txt åŠ è½½å®éªŒå‚æ•°
    
    æ ¼å¼: pid|exp_name|algorithm|gpu_id|extra_params|timestamp
    """
    tasks_file = os.path.join(log_dir, "running_tasks.txt")
    params_map = {}
    
    if not os.path.exists(tasks_file):
        return params_map
    
    try:
        with open(tasks_file, "r") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split("|")
                if len(parts) >= 5:
                    exp_name = parts[1]
                    extra_params = parts[4]
                    
                    # è§£æå‚æ•°å­—ç¬¦ä¸²ä¸ºå­—å…¸
                    hp_dict = {}
                    # åŒ¹é… --key value æ ¼å¼ï¼Œæ”¯æŒç§‘å­¦è®°æ•°æ³•ï¼ˆå¦‚ 1e-4ï¼‰
                    # ä½¿ç”¨ä¸¤æ­¥è§£æï¼šå…ˆåˆ†å‰²å‚æ•°ï¼Œå†è§£ææ¯ä¸ªé”®å€¼å¯¹
                    tokens = extra_params.split()
                    i = 0
                    while i < len(tokens):
                        if tokens[i].startswith("--"):
                            key = tokens[i][2:]  # å»æ‰ --
                            if i + 1 < len(tokens) and not tokens[i + 1].startswith("--"):
                                value = tokens[i + 1]
                                try:
                                    # å°è¯•è½¬æ¢ä¸ºæ•°å­—ï¼ˆæ”¯æŒç§‘å­¦è®°æ•°æ³•ï¼‰
                                    if "." in value or "e" in value.lower():
                                        hp_dict[key] = float(value)
                                    else:
                                        hp_dict[key] = int(value)
                                except ValueError:
                                    hp_dict[key] = value
                                i += 2
                            else:
                                # å¸ƒå°”æ ‡å¿—
                                hp_dict[key] = True
                                i += 1
                        else:
                            i += 1
                    
                    params_map[exp_name] = hp_dict
    except Exception as e:
        print(f"   âš ï¸ è¯»å– running_tasks.txt å¤±è´¥: {e}")
    
    return params_map


def collect_from_logs(log_dir: str, recursive: bool = False) -> pd.DataFrame:
    """ä»è®­ç»ƒæ—¥å¿—æ–‡ä»¶æ”¶é›†ç»“æœ"""
    print(f"ğŸ“‚ ä»æ—¥å¿—ç›®å½•æ”¶é›†: {log_dir}")
    
    results = []
    
    if recursive:
        log_files = glob.glob(os.path.join(log_dir, "**/*.log"), recursive=True)
    else:
        log_files = glob.glob(os.path.join(log_dir, "*.log"))
    
    # åŠ è½½å‚æ•°æ˜ å°„ï¼ˆä» running_tasks.txtï¼‰
    # å¯¹äºé€’å½’æ¨¡å¼ï¼Œå°è¯•ä»æ¯ä¸ªå­ç›®å½•åŠ è½½
    all_params_map = {}
    if recursive:
        for log_file in log_files:
            parent_dir = os.path.dirname(log_file)
            if parent_dir not in all_params_map:
                params = load_running_tasks(parent_dir)
                all_params_map.update(params)
    else:
        all_params_map = load_running_tasks(log_dir)
    
    for log_file in log_files:
        exp_name = Path(log_file).stem
        
        # è§£æå®éªŒåç§°: task_algo_cfgN_obsmode
        match = re.match(r"(.+?)_([a-z_]+)_cfg(\d+)_(.+)", exp_name)
        if not match:
            continue
            
        task, algo, cfg_idx, obs_mode = match.groups()
        
        # è§£ææ—¥å¿—æ–‡ä»¶
        metrics = parse_log_file(log_file)
        if metrics:
            metrics.update({
                "exp_name": exp_name,
                "task": task,
                "algorithm": algo,
                "config_idx": int(cfg_idx),
                "obs_mode": obs_mode,
                "log_file": log_file
            })
            
            # æ·»åŠ ä» running_tasks.txt è¯»å–çš„è¶…å‚æ•°
            if exp_name in all_params_map:
                for key, value in all_params_map[exp_name].items():
                    metrics[f"hp_{key}"] = value
            
            results.append(metrics)
    
    df = pd.DataFrame(results)
    print(f"   æ”¶é›†åˆ° {len(df)} ä¸ªå®éªŒç»“æœ")
    return df


def parse_log_file(log_file: str) -> Optional[Dict[str, Any]]:
    """è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œæå–æœ€ç»ˆæŒ‡æ ‡"""
    try:
        with open(log_file, "r") as f:
            content = f.read()
        
        metrics = {}
        
        # å¸¸è§æŒ‡æ ‡çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = {
            "eval/success_once": r"eval/success_once[:\s]+([0-9.]+)",
            "eval/success_at_end": r"eval/success_at_end[:\s]+([0-9.]+)",
            "eval/return": r"eval/return[:\s]+([0-9.-]+)",
            "train/loss": r"train/loss[:\s]+([0-9.]+)",
            "final_success": r"Final success.*?([0-9.]+)",
            "best_success": r"Best success.*?([0-9.]+)",
        }
        
        for metric_name, pattern in patterns.items():
            # æ‰¾æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆæœ€ç»ˆå€¼ï¼‰
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                try:
                    metrics[metric_name] = float(matches[-1])
                except ValueError:
                    pass
        
        # æ£€æŸ¥è®­ç»ƒæ˜¯å¦å®Œæˆ
        if "Training completed" in content or "Final" in content:
            metrics["completed"] = True
        else:
            metrics["completed"] = False
            
        # æå–è¶…å‚æ•°ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°ï¼‰
        hp_patterns = {
            "lr": r"--lr\s+([0-9.e-]+)",
            "beta": r"--beta\s+([0-9.]+)",
            "alpha": r"--alpha\s+([0-9.]+)",
            "consistency_weight": r"--consistency_weight\s+([0-9.]+)",
            "shortcut_weight": r"--shortcut_weight\s+([0-9.]+)",
            "num_flow_steps": r"--num_flow_steps\s+(\d+)",
            "num_diffusion_iters": r"--num_diffusion_iters\s+(\d+)",
            "reward_scale": r"--reward_scale\s+([0-9.]+)",
            "sc_num_inference_steps": r"--sc_num_inference_steps\s+(\d+)",
        }
        
        for hp_name, pattern in hp_patterns.items():
            match = re.search(pattern, content)
            if match:
                val = match.group(1)
                try:
                    metrics[f"hp_{hp_name}"] = float(val) if "." in val or "e" in val else int(val)
                except ValueError:
                    metrics[f"hp_{hp_name}"] = val
        
        return metrics if metrics else None
        
    except Exception as e:
        print(f"   âš ï¸ è§£æå¤±è´¥ {log_file}: {e}")
        return None


def collect_from_wandb(project: str, entity: Optional[str] = None) -> pd.DataFrame:
    """ä» WandB æ”¶é›†ç»“æœ"""
    print(f"ğŸ“¡ ä» WandB æ”¶é›†: {project}")
    
    try:
        import wandb
        api = wandb.Api()
        
        # è·å–æ‰€æœ‰ runs
        path = f"{entity}/{project}" if entity else project
        runs = api.runs(path)
        
        results = []
        for run in runs:
            # æå–é…ç½®å’ŒæŒ‡æ ‡
            config = run.config
            summary = run.summary._json_dict
            
            # è§£æå®éªŒåç§°
            exp_name = run.name
            match = re.match(r"(.+?)_([a-z_]+)_cfg(\d+)_(.+)", exp_name)
            if not match:
                continue
            
            task, algo, cfg_idx, obs_mode = match.groups()
            
            metrics = {
                "exp_name": exp_name,
                "task": task,
                "algorithm": algo,
                "config_idx": int(cfg_idx),
                "obs_mode": obs_mode,
                "run_id": run.id,
                "state": run.state,
            }
            
            # æ·»åŠ å…³é”®æŒ‡æ ‡
            for key in ["eval/success_once", "eval/success_at_end", "eval/return"]:
                if key in summary:
                    metrics[key] = summary[key]
            
            # æ·»åŠ è¶…å‚æ•°
            for key, val in config.items():
                metrics[f"hp_{key}"] = val
            
            results.append(metrics)
        
        df = pd.DataFrame(results)
        print(f"   æ”¶é›†åˆ° {len(df)} ä¸ªå®éªŒç»“æœ")
        return df
        
    except ImportError:
        print("   âš ï¸ wandb æœªå®‰è£…")
        return pd.DataFrame()
    except Exception as e:
        print(f"   âš ï¸ WandB æ”¶é›†å¤±è´¥: {e}")
        return pd.DataFrame()


def collect_from_tensorboard(log_dir: str) -> pd.DataFrame:
    """ä» TensorBoard æ—¥å¿—æ”¶é›†ç»“æœ"""
    print(f"ğŸ“Š ä» TensorBoard æ”¶é›†: {log_dir}")
    
    try:
        from tensorboard.backend.event_processing import event_accumulator
        
        results = []
        tb_dirs = glob.glob(os.path.join(log_dir, "**/events.out.tfevents.*"), recursive=True)
        
        for tb_file in tb_dirs:
            tb_dir = os.path.dirname(tb_file)
            exp_name = os.path.basename(tb_dir)
            
            try:
                ea = event_accumulator.EventAccumulator(tb_dir)
                ea.Reload()
                
                metrics = {"exp_name": exp_name}
                
                # è¯»å–æ ‡é‡
                for tag in ea.Tags()["scalars"]:
                    events = ea.Scalars(tag)
                    if events:
                        # å–æœ€åä¸€ä¸ªå€¼
                        metrics[tag.replace("/", "_")] = events[-1].value
                
                results.append(metrics)
            except Exception:
                continue
        
        df = pd.DataFrame(results)
        print(f"   æ”¶é›†åˆ° {len(df)} ä¸ªå®éªŒç»“æœ")
        return df
        
    except ImportError:
        print("   âš ï¸ tensorboard æœªå®‰è£…")
        return pd.DataFrame()


# =============================================================================
# åˆ†æ
# =============================================================================

def analyze_results(
    df: pd.DataFrame,
    metric: str = "eval/success_once",
    secondary_metrics: List[str] = None,
    top_k: int = 3
) -> Dict[str, Any]:
    """åˆ†æç»“æœï¼Œä¸ºæ¯ä¸ªç®—æ³•æ‰¾åˆ°æœ€ä¼˜é…ç½®"""
    
    if df.empty:
        return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
    
    results = {
        "summary": {},
        "best_per_algorithm": {},
        "all_results": {}
    }
    
    # æŒ‰ç®—æ³•åˆ†ç»„
    algorithms = df["algorithm"].unique()
    
    for algo in algorithms:
        algo_df = df[df["algorithm"] == algo].copy()
        
        if metric not in algo_df.columns:
            continue
        
        # æŒ‰ä¸»æŒ‡æ ‡æ’åº
        algo_df_sorted = algo_df.sort_values(metric, ascending=False)
        
        # è·å– top K
        top_k_rows = algo_df_sorted.head(top_k)
        
        # æ‰¾å‡ºè¶…å‚æ•°åˆ—
        hp_cols = [c for c in algo_df.columns if c.startswith("hp_")]
        
        best_config = top_k_rows.iloc[0] if len(top_k_rows) > 0 else None
        
        results["best_per_algorithm"][algo] = {
            "best_metric": float(best_config[metric]) if best_config is not None else None,
            "best_config": {
                col.replace("hp_", ""): best_config[col] 
                for col in hp_cols 
                if pd.notna(best_config[col])
            } if best_config is not None else {},
            "top_k": [
                {
                    "exp_name": row["exp_name"],
                    metric: float(row[metric]) if pd.notna(row[metric]) else None,
                    **{m: float(row[m]) if m in row and pd.notna(row[m]) else None 
                       for m in (secondary_metrics or [])},
                    "config": {
                        col.replace("hp_", ""): row[col] 
                        for col in hp_cols 
                        if pd.notna(row[col])
                    }
                }
                for _, row in top_k_rows.iterrows()
            ],
            "num_experiments": len(algo_df)
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        results["summary"][algo] = {
            "count": len(algo_df),
            "mean": float(algo_df[metric].mean()) if metric in algo_df else None,
            "std": float(algo_df[metric].std()) if metric in algo_df else None,
            "max": float(algo_df[metric].max()) if metric in algo_df else None,
            "min": float(algo_df[metric].min()) if metric in algo_df else None,
        }
    
    # å…¨å±€æœ€ä½³
    if metric in df.columns:
        best_overall = df.loc[df[metric].idxmax()]
        results["best_overall"] = {
            "algorithm": best_overall["algorithm"],
            "exp_name": best_overall["exp_name"],
            "metric": float(best_overall[metric])
        }
    
    return results


def generate_recommendations(analysis: Dict[str, Any]) -> List[str]:
    """åŸºäºåˆ†æç»“æœç”Ÿæˆæ¨è"""
    recommendations = []
    
    if "best_overall" in analysis:
        best = analysis["best_overall"]
        recommendations.append(
            f"ğŸ† å…¨å±€æœ€ä½³: {best['algorithm']} è¾¾åˆ° {best['metric']:.4f}"
        )
    
    # æŒ‰ç®—æ³•æ€§èƒ½æ’åº
    algo_scores = {
        algo: data["best_metric"]
        for algo, data in analysis.get("best_per_algorithm", {}).items()
        if data["best_metric"] is not None
    }
    sorted_algos = sorted(algo_scores.items(), key=lambda x: x[1], reverse=True)
    
    recommendations.append("\nğŸ“Š ç®—æ³•æ’å (æŒ‰æœ€ä½³é…ç½®è¡¨ç°):")
    for rank, (algo, score) in enumerate(sorted_algos, 1):
        recommendations.append(f"   {rank}. {algo}: {score:.4f}")
    
    # è¶…å‚æ•°å»ºè®®
    recommendations.append("\nğŸ’¡ æœ€ä¼˜è¶…å‚æ•°å»ºè®®:")
    for algo, data in analysis.get("best_per_algorithm", {}).items():
        config = data.get("best_config", {})
        if config:
            config_str = ", ".join(f"{k}={v}" for k, v in config.items())
            recommendations.append(f"   {algo}: {config_str}")
    
    return recommendations


# =============================================================================
# è¾“å‡º
# =============================================================================

def print_table(analysis: Dict[str, Any], metric: str):
    """æ‰“å°è¡¨æ ¼å½¢å¼çš„ç»“æœ"""
    print("\n" + "=" * 80)
    print("è¶…å‚æ•°æ‰«æç»“æœåˆ†æ")
    print("=" * 80)
    
    # ç»Ÿè®¡è¡¨æ ¼
    print(f"\nğŸ“ˆ ç®—æ³•ç»Ÿè®¡ (æŒ‡æ ‡: {metric})")
    print("-" * 60)
    print(f"{'ç®—æ³•':<25} {'å®éªŒæ•°':<8} {'æœ€ä½³':<10} {'å¹³å‡':<10} {'æ ‡å‡†å·®':<10}")
    print("-" * 60)
    
    for algo, stats in sorted(analysis.get("summary", {}).items()):
        best = stats.get("max", 0) or 0
        mean = stats.get("mean", 0) or 0
        std = stats.get("std", 0) or 0
        count = stats.get("count", 0)
        print(f"{algo:<25} {count:<8} {best:<10.4f} {mean:<10.4f} {std:<10.4f}")
    
    # æœ€ä½³é…ç½®è¡¨æ ¼
    print(f"\nğŸ† å„ç®—æ³•æœ€ä½³é…ç½®")
    print("-" * 80)
    
    for algo, data in analysis.get("best_per_algorithm", {}).items():
        print(f"\nã€{algo}ã€‘ æœ€ä½³: {data.get('best_metric', 'N/A'):.4f}")
        config = data.get("best_config", {})
        if config:
            print(f"   é…ç½®: {config}")
        
        print(f"   Top é…ç½®:")
        for i, entry in enumerate(data.get("top_k", []), 1):
            entry_metric = entry.get(metric, "N/A")
            if isinstance(entry_metric, (int, float)):
                print(f"      {i}. {entry_metric:.4f} - {entry.get('config', {})}")
    
    # æ¨è
    recommendations = generate_recommendations(analysis)
    print("\n" + "\n".join(recommendations))


def save_json(analysis: Dict[str, Any], output_path: str):
    """ä¿å­˜ JSON æ ¼å¼ç»“æœ"""
    with open(output_path, "w") as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
    print(f"ğŸ“„ JSON å·²ä¿å­˜: {output_path}")


def save_csv(df: pd.DataFrame, output_path: str):
    """ä¿å­˜ CSV æ ¼å¼ç»“æœ"""
    df.to_csv(output_path, index=False)
    print(f"ğŸ“„ CSV å·²ä¿å­˜: {output_path}")


def save_markdown_report(
    analysis: Dict[str, Any], 
    df: pd.DataFrame, 
    metric: str,
    output_path: str
):
    """ç”Ÿæˆ Markdown åˆ†ææŠ¥å‘Š"""
    lines = ["# è¶…å‚æ•°æ‰«æåˆ†ææŠ¥å‘Š\n"]
    
    # æ¦‚è§ˆ
    lines.append("## ğŸ“Š æ¦‚è§ˆ\n")
    lines.append(f"- æ€»å®éªŒæ•°: {len(df)}")
    lines.append(f"- ç®—æ³•æ•°: {len(analysis.get('summary', {}))}")
    lines.append(f"- ä¸»æŒ‡æ ‡: `{metric}`")
    
    if "best_overall" in analysis:
        best = analysis["best_overall"]
        lines.append(f"\n**ğŸ† å…¨å±€æœ€ä½³**: {best['algorithm']} ({best['metric']:.4f})\n")
    
    # ç»Ÿè®¡è¡¨æ ¼
    lines.append("\n## ğŸ“ˆ ç®—æ³•æ€§èƒ½ç»Ÿè®¡\n")
    lines.append("| ç®—æ³• | å®éªŒæ•° | æœ€ä½³ | å¹³å‡ | æ ‡å‡†å·® |")
    lines.append("|------|--------|------|------|--------|")
    
    for algo, stats in sorted(
        analysis.get("summary", {}).items(), 
        key=lambda x: x[1].get("max", 0) or 0,
        reverse=True
    ):
        best = stats.get("max", 0) or 0
        mean = stats.get("mean", 0) or 0
        std = stats.get("std", 0) or 0
        count = stats.get("count", 0)
        lines.append(f"| {algo} | {count} | {best:.4f} | {mean:.4f} | {std:.4f} |")
    
    # æœ€ä½³é…ç½®
    lines.append("\n## ğŸ¯ å„ç®—æ³•æœ€ä½³é…ç½®\n")
    
    for algo, data in analysis.get("best_per_algorithm", {}).items():
        lines.append(f"### {algo}\n")
        lines.append(f"- æœ€ä½³æŒ‡æ ‡: **{data.get('best_metric', 'N/A'):.4f}**")
        
        config = data.get("best_config", {})
        if config:
            config_str = ", ".join(f"`{k}={v}`" for k, v in config.items())
            lines.append(f"- æœ€ä½³é…ç½®: {config_str}")
        
        lines.append(f"\n**Top é…ç½®:**\n")
        lines.append("| æ’å | æŒ‡æ ‡ | é…ç½® |")
        lines.append("|------|------|------|")
        for i, entry in enumerate(data.get("top_k", []), 1):
            entry_metric = entry.get(metric, "N/A")
            if isinstance(entry_metric, (int, float)):
                cfg_str = ", ".join(f"{k}={v}" for k, v in entry.get("config", {}).items())
                lines.append(f"| {i} | {entry_metric:.4f} | {cfg_str} |")
        lines.append("")
    
    # æ¨è
    lines.append("\n## ğŸ’¡ æ¨è\n")
    recommendations = generate_recommendations(analysis)
    for rec in recommendations:
        lines.append(rec)
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, "w") as f:
        f.write("\n".join(lines))
    print(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def export_best_params(
    analysis: Dict[str, Any],
    output_dir: str,
    algorithm: Optional[str] = None
):
    """å¯¼å‡ºæœ€ä¼˜å‚æ•°ä¸º shell è„šæœ¬ï¼Œä¾›ä¸‹æ¸¸ç®—æ³•ç»§æ‰¿
    
    è¾“å‡ºæ ¼å¼:
    # best_params_flow_matching.sh
    BEST_LR="3e-4"
    BEST_NUM_FLOW_STEPS="10"
    BEST_OBS_HORIZON="2"
    ...
    """
    algos_to_export = [algorithm] if algorithm else analysis.get("best_per_algorithm", {}).keys()
    
    for algo in algos_to_export:
        if algo not in analysis.get("best_per_algorithm", {}):
            print(f"âš ï¸ ç®—æ³• {algo} æ— ç»“æœï¼Œè·³è¿‡å¯¼å‡º")
            continue
        
        data = analysis["best_per_algorithm"][algo]
        config = data.get("best_config", {})
        
        if not config:
            print(f"âš ï¸ ç®—æ³• {algo} æ— æœ€ä¼˜é…ç½®ï¼Œè·³è¿‡å¯¼å‡º")
            continue
        
        output_path = os.path.join(output_dir, f"best_params_{algo}.sh")
        
        lines = [
            f"# æœ€ä¼˜å‚æ•°: {algo}",
            f"# æœ€ä½³æŒ‡æ ‡: {data.get('best_metric', 'N/A')}",
            f"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ]
        
        for key, value in config.items():
            # è½¬æ¢ä¸ºå¤§å†™ç¯å¢ƒå˜é‡æ ¼å¼
            env_key = f"BEST_{key.upper()}"
            # ç¡®ä¿å€¼è¢«å¼•å·åŒ…å›´
            if isinstance(value, str):
                lines.append(f'{env_key}="{value}"')
            else:
                lines.append(f'{env_key}="{value}"')
        
        with open(output_path, "w") as f:
            f.write("\n".join(lines) + "\n")
        
        print(f"ğŸ“„ æœ€ä¼˜å‚æ•°å·²å¯¼å‡º: {output_path}")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    args = parse_args()
    
    print("=" * 60)
    print("è¶…å‚æ•°æ‰«æç»“æœåˆ†æå™¨")
    print("=" * 60)
    
    # æ”¶é›†æ•°æ®
    dfs = []
    
    if args.log_dir:
        df_logs = collect_from_logs(args.log_dir, recursive=args.recursive)
        if not df_logs.empty:
            dfs.append(df_logs)
        
        df_tb = collect_from_tensorboard(args.log_dir)
        if not df_tb.empty:
            dfs.append(df_tb)
    
    if args.wandb_project:
        df_wandb = collect_from_wandb(args.wandb_project, args.wandb_entity)
        if not df_wandb.empty:
            dfs.append(df_wandb)
    
    if not dfs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
        return
    
    # åˆå¹¶æ•°æ®
    df = pd.concat(dfs, ignore_index=True)
    df = df.drop_duplicates(subset=["exp_name"], keep="last")
    
    # å¦‚æœæŒ‡å®šäº†ç®—æ³•ï¼Œè¿‡æ»¤æ•°æ®
    if args.algorithm:
        df = df[df["algorithm"] == args.algorithm]
        if df.empty:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ç®—æ³• {args.algorithm} çš„æ•°æ®")
            return
    
    print(f"\nğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(df)} ä¸ªå®éªŒç»“æœ")
    print(f"   ç®—æ³•: {df['algorithm'].unique().tolist()}")
    
    # åˆ†æ
    analysis = analyze_results(
        df,
        metric=args.metric,
        secondary_metrics=args.secondary_metrics,
        top_k=args.top_k
    )
    
    # è¾“å‡º
    output_dir = args.output_dir or args.log_dir or "."
    os.makedirs(output_dir, exist_ok=True)
    
    if args.format in ["table", "all"]:
        print_table(analysis, args.metric)
    
    if args.format in ["json", "all"]:
        save_json(analysis, os.path.join(output_dir, "sweep_analysis.json"))
    
    if args.format in ["csv", "all"]:
        save_csv(df, os.path.join(output_dir, "sweep_results.csv"))
    
    if args.format == "all":
        save_markdown_report(
            analysis, df, args.metric,
            os.path.join(output_dir, "sweep_report.md")
        )
    
    # å¯¼å‡ºæœ€ä¼˜å‚æ•°
    if args.export_best:
        export_best_params(analysis, output_dir, args.algorithm)
    
    print("\nâœ… åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    main()
